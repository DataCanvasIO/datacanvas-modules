# Update

## 180504 

* [A] [AdaboostClasSPy3](#AboostC)
* [A] [AdaboostRegrSPy3](#AboostR)
* [A] [BaggingClasSPy3](#BaggC)
* [A] [BaggingRegrSPy3](#BaggR)
* [A] [ChangeTypeDataSPy3](#CTypeD)
* [A] [ClasEvalSPy3](#CEval)
* [A] [ClasPredictSPy3](#CPredict)
* [A] [ClasRocEvalSPy3a](#CREval)
* [A] [ColsDropDataSPy3](#CDropD)
* [A] [ColsSelectCSVSPy3](#CSelectCSV) 
* [A] [ColsSelectDataSPy3_2](#CSelect2D)
* [A] [ColsSelect2DataSPy3](#CSelect2Data)
* [A] [CorrXXFeatSpy3](#CXXF) 
* [A] [CorrXYFeatSPy3](#CXYF)
* [A] [DataDownloaderUnivSPy3](#DDownU)
* [A] [DataInfoUnivSPy3](#DInfoU)
* [A] [DummyFitDataSPy3](#DFitD)
* [A] [DummyTransformDataSPy3](#DTransformD)
* [A] [ExtratreesClasSPy3](#ExtratreeC)
* [A] [ExtratreesRegrSPy3](#ExtratreeR)
* [A] [FillNADataSPy3](#FNAD)
* [A] [FormShowUnivSPy3](#FShowU)
* [A] [FormShowCSVUnivSPy3](#FShowCSVU)
* [A] [GradientboostingClasSPy3](#Gboosting)
* [A] [GradientboostingRegrSPy3](#GboostingR)
* [A] [LogisticRegrSPy3](#LogisticR)
* [A] [MinMaxScalerFitDataSPy3](#MMSFitD)
* [A] [MinMaxScalerTransformDataSPy3](#MMSTransformD)
* [M] [MissingDropDataSPy3](#MDropD)
* [M] [MissingFillDataSPy3](#MFillD)
* [A] [MissingImputeDataSPy3](#MImputeD)
* [A] [PlotLearningCurveSPy3](#PLCurve)
* [A] [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [A] [PmmlClasSPy3](#PmmlC)
* [A] [RandomforestClasSPy3](#Rforest)
* [A] [RandomforestRegrSPy3](#RforestR)
* [A] [RegrEvalSPy3](#REvalS)
* [A] [ReportPDFClasEvalSPy3](#RPDFCE)
* [A] [RFEFeatSPy3](#RFEF)
* [A] [SplitFeatSPy3](#SplitF)
* [A] [VarianceThresholdFitFeatSPy3](#VTFitF)
* [A] [VarianceThresholdTransformFeatSPy3](#VTTransformF)


## 180411

* [A] [GetSparkSessionUnivDPy3](#GetSpark)
* [A] [FeatureSelectorDataDPy3](#FSSpark)
* [A] [FeatureTransformerFeatDPy3](#FTSpark)
* [A] [DecisionTreeClasDPy3](#DTtrainSpark)
* [A] [DecisionTreeEvalDPy3](#DTevalSpark)
* [A] [CloseSparkSessionUnivDPy3](#CloseSpark)

## 180410

* [A] [MissingDropDataSPy3](#MDropD)
* [A] [MissingFillDataSPy3](#MFillD)
* [A] [SampleDataSPy3](#sample)
* [A] [BoxDataSPy3](#box)
* [A] [MinmaxScalerDataSPy3](#MM)
* [A] [StandardScalerDataSPy3](#sc)
* [A] [Chi2CorrFeatSPy3](#chi)

## 180223

* [A] [VariablesSelection](#VS)
* [A] [DataPreprocessing](#DP)
* [A] [DataTypes](#DataT)
* [A] [MissingCheck](#MCheck)
* [A] [AsType](#AsT)
* [A] [ClassMapping](#classmap)
* [A] [QuantileTransformer](#QTrans)
* [A] [Box](#box)
* [A] [SplitXY](#split)
* [A] [chi2](#chi) 
* [A] [PearsonCorrelation](#pearson)
* [A] [SelectFromModel](#sfm)
* [A] [Union](*union)
* [A] [Imbalance](#imba)
* [A] [TrainTestSplit](#split)
* [A] [Stacking](#stack)
* [A] [ConfusionMatrix](#cnf)
* [M] [AdaBoost](#Ada)
* [A] [Prediction](#pred)

## 180209

* [A] [Sample](#sample)
* [A] [ChurnLabel](#CL)
* [A] [ValueCounts](#VC)n
* [A] [BucketLowFrequency](#BLF) 

## 180205

* [A] [HashingEncoder](#he)
* [A] [AdaBoost](#Ada)
* [A] [xgboost](#xg)
* [A] [StandardScaler](#sc)
* [A] [SelectFromModel](#sfm)
* [A] [PearsonCorrelation](#pearson)
* [A] [RFE](#rfe)
* [A] [MINE](#mine)
* [A] [chi2](#chi) 
* [A] [FunctionTransformer](#ft)
* [A] [PolyNomialFeatures](#poly)


# Index

## calibration 概率检验

## cluster 聚类

## covariance 协方差估计

## data_retrieve 从数据源获取数据

* [DataDownloaderUnivSPy3](#DDownU)

## dataframe 数据操作

* [ChangeTypeDataSPy3](#CTypeD)
* [ColsDropDataSPy3](#CDropD)
* [ColsSelectCSVSPy3](#CSelectCSV) 
* [ColsSelectDataSPy3_2](#CSelect2D) 
* [ColsSelect2DataSPy3](#CSelect2Data) 
* [DataInfoUnivSPy3](#DInfoU)
* [FeatureSelectorDataDPy3](#FSSpark)
* [FillNADataSPy3](#FNAD)
* [ValueCounts](#VC)
* [BucketLowFrequency](#BLF) 
* [VariablesSelection](#VS)
* [DataTypes](#DataT)
* [MissingCheck](#MCheck)
* [SplitXY](#split)
 
## decomposition 矩阵分解

## discriminant_analysis 判别分析

## ensemble 集成方法

* [AdaboostClasSPy3](#AboostC)
* [AdaboostRegrSPy3](#AboostR)
* [BaggingClasSPy3](#BaggC)
* [BaggingRegrSPy3](#BaggR)
* [ExtratreesClasSPy3](#ExtratreeC)
* [ExtratreesRegrSPy3](#ExtratreeR)
* [GradientboostingClasSPy3](#Gboosting)
* [GradientboostingRegrSPy3](#GboostingR)
* [RandomforestClasSPy3](#Rforest)
* [RandomforestRegrSPy3](#RforestR)
* [AdaBoost](#Ada)
* [xgboost](#xg)
* [Stacking](#stack)

## feature_extraction 特征提取

## feature_selection 特征选择

* [CorrXXFeatSpy3](#CXXF)
* [CorrXYFeatSPy3](#CXYF)
* [RFEFeatSPy3](#RFEF)
* [VarianceThresholdFitFeatSPy3](#VTFitF)
* [VarianceThresholdTransformFeatSPy3](#VTTransformF)
* [SelectFromModel](#sfm)
* [PearsonCorrelation](#pearson)
* [MINE](#mine)
* [chi2](#chi) 
* [Union](*union)

## linear_model 线性模型
* [LogisticRegrSPy3](#LogisticR)

## metrics 评估指标
* [ClasEvalSPy3](#CEval)
* [ClasPredictSPy3](#CPredict)
* [ConfusionMatrix](#cnf)
* [RegrEvalSPy3](#REvalS)
* [ReportPDFClasEvalSPy3](#RPDFCE)
* [Prediction](#pred)

## model_selection 模型选择
* [SplitFeatSPy3](#SplitF)

## multiclass 多类和多标签分类

## naive_bayes 朴素贝叶斯

## neighbors 最近邻算法

## neural_network 神经网络

## pipeline 工作管线

## preprocessing 预处理和正则化

* [DummyFitDataSPy3](#DFitD)
* [DummyTransformDataSPy3](#DTransformD)
* [MinMaxScalerFitDataSPy3](#MMSFitD)
* [MinMaxScalerTransformDataSPy3](#MMSTransformD)
* [MissingDropDataSPy3](#MDropD)
* [MissingFillDataSPy3](#MFillD)
* [MissingImputeDataSPy3](#MImputeD)
* [HashingEncoder](#he)
* [StandardScaler](#sc)
* [StandardScalerDataSPy3](#sc)
* [FunctionTransformer](#ft)
* [MinmaxScalerDataSPy3](#MM)
* [PolyNomialFeatures](#poly)
* [Sample](#sample)
* [SampleDataSPy3](#sample)
* [DataPreprocessing](#DP)
* [AsType](#AsT)
* [ClassMapping](#classmap)
* [QuantileTransformer](#QTrans)
* [Box](#box)
* [Imbalance](#imba)
* [FeatureTransformerFeatDPy3](#FTSpark)

## pyspark 分布式
* [GetSparkSessionUnivDPy3](#GetSpark)
* [FeatureSelectorDataDPy3](#FSSpark)
* [FeatureTransformerFeatDPy3](#FTSpark)
* [DecisionTreeClasDPy3](#DTtrainSpark)
* [DecisionTreeEvalDPy3](#DTevalSpark)
* [CloseSparkSessionUnivDPy3](#CloseSpark)

## svm 支持向量机模型

## tree 树模型
* [DecisionTreeClasDPy3](#DTtrainSpark)
* [DecisionTreeEvalDPy3](#DTevalSpark)

## visualization 可视化
* [ClasRocEvalSPy3a](#CREval)
* [FormShowUnivSPy3](#FShowU)
* [FormShowCSVUnivSPy3](#FShowCSVU)
* [PlotLearningCurveSPy3](#PLCurve)
* [PlotLearningCurveSPy3_BestModel](#PLCBest)
* [ReportPDFClasEvalSPy3](#RPDFCE)

## utils 通用工具
* [PmmlClasSPy3](#PmmlC)


# Module

## <a id="AboostC">AdaboostClasSPy3</a>
一种对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来构成一个更强的最终分类器(强分类器)的迭代算法。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="AboostR">AdaboostRegrSPy3</a>
一种对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来构成一个更强的最终分类器(强分类器)的迭代算法。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。使用adaboost分类器可以排除一些不必要的训练数据特征，并放在关键的训练数据上面。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_import_feat (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="BaggC">BaggingClasSPy3</a>
bagging是一种用来提高学习算法准确度的方法，这种方法通过构造一个预测函数系列，然后以一定的方式将它们组合成一个预测函数。Bagging要求“不稳定”（不稳定是指数据集的小的变动能够使得分类结果的显著的变动）的分类方法。比如：决策树，神经网络算法。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* max_samples (double): 最大采样比率
* max_features (double): 最大特征比率

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="BaggR">BaggingRegrSPy3</a>
bagging是一种用来提高学习算法准确度的方法，这种方法通过构造一个预测函数系列，然后以一定的方式将它们组合成一个预测函数。Bagging要求“不稳定”（不稳定是指数据集的小的变动能够使得分类结果的显著的变动）的分类方法。比如：决策树，神经网络算法。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* max_samples (double): 最大采样比率

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="CTypeD">ChangeTypeDataSPy3</a>
转换指定列的数据类型

#### Tag:

* dataframe

#### Param:

* cols (string): 选择要转换类型的变量
* type (string): 转换后的类型(object, int64, float64) 

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl)： 指定列转换类型后的数据


## <a id="CEval">ClasEvalSPy3</a>
对二分类及多分类模型进行评估（包括AUC，Kappa，评估报告及混淆矩阵等）

#### Tag:

* metrics

#### Param:

* None

#### Input:

* d_true (csv): 真实标签
* d_pred (csv): 预测的标签变量
* d_prob (csv): 预测概率

#### Output:

* o_metric (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix (jpg): 混淆矩阵图


## <a id="CPredict">ClasPredictSPy3</a>
通过已训练好的模型进行预测

#### Tag:

* metrics

#### Param:

* None

#### Input:

* d_feature (csv): 特征变量
* m_fitted_model (py3pkl): 算法训练好的模型

#### Output:

* d_predict (csv): 预测值及预测概率
* d_pred (csv): 预测值
* d_prob (csv): 预测概率


## <a id="CREval">ClasRocEvalSPy3a</a>
输出分类模型的ROC曲线

#### Tag:

* visualization

#### Param:

* None

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量
* m_fitted_model (py3pkl): 训练好的模型

#### Output:

* o_roc_curve (jpg): ROC曲线图


## <a id="CDropD">ColsDropDataSPy3</a>
删除指定列

#### Tag:

* dataframe

#### Param:

* cols (string): 选择要删除的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_changed_data (py3pkl): 删除指定列后的数据


## <a id="CSelectCSV">ColsSelectCSVSPy3</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* cols (string): 选择需要的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_selected_data (csv): 变量选择后的dataframe


## <a id="CSelect2D">ColsSelectDataSPy3_2</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* cols (string): 选择需要的变量

#### Input:

* d_data (csv): 数据

#### Output:

* d_selected_data (py3pkl): 变量选择后的dataframe


## <a id="CSelect2Data">ColsSelect2DataSPy3</a>
从dataframe选择需要的变量

#### Tag:

* dataframe

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* selected_cols (py3pkl): 选择的变量

#### Output:

* d_selected_data (csv)： 选择变量后的dataframe


## <a id="CXXF">CorrXXFeatSpy3</a>
计算特征变量和特征变量之间的(pearson/spearman/kendall)相关性，并通过设定的参数来消除强相关的特征变量

#### Tag:

* feature_selection

#### Param:

* corr_type (string): 计算相关性方法
* corr_threshold (double): 消除强相关变量的阈值

#### Input:

* d_feature (csv): 数据
* o_featrue_label_corr (csv): 与标签变量间的相关性分数 (chi2/互信息/F检验分数)

#### Output:

* d_feature_selected (csv): 相关性筛选后的数据
* o_corr_XX (html): 相关性矩阵
* o_corr_heatmap (jpg): 相关性热力图


## <a id="CXYF">CorrXYFeatSPy3</a>
计算特征变量和标签变量之间的相关性（卡方/互信息/F检验），并通过设定的参数来筛选相应的特征变量

#### Tag:

* feature_selection

#### Param:

* feature_percent (int): 保留变量个数百分比 (0-100)
* sample_rate (double): 抽样比例 (0-1)

#### Input:

* d_feature (py3pkl): 目标变量
* d_label (py3pkl): 标签变量

#### Output:

* d_feature_selected (csv): 相关性筛选后的数据
* o_featrue_label_corr (csv): 与标签变量间的相关性分数 (卡方/互信息/F检验分数)


## <a id="DDownU">DataDownloaderUnivSPy3</a>
解析数据模块表示的资源引用，然后导入工作流。

#### Tag:

* data_retrieve

#### Param:

* None

#### Input:

* data_source (datasource.file): 从数据模块获取到的路径

#### Output:

* data (any): 数据


## <a id="DInfoU">DataInfoUnivSPy3</a>
数据统计：数据类型与缺失值比例； 均值，标准差，最小值，25分位数，50分位数，75分位数，最大值

#### Tag:

* dataframe

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* o_data_type_null (html): 数据类型与缺失值统计
* o_data_describe (html): 均值，标准差，最小值，25分位数，50分位数，75分位数，最大值


## <a id="DFitD">DummyFitDataSPy3</a>
对数据做哑编码转化(训练数据时使用，针对类别型变量)

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_dummy_data (py3pkl): 哑编码后的数据
* cols (py3pkl): 哑编码后的所有变量


## <a id="DTransformD">DummyTransformDataSPy3</a>
对数据做哑编码转化(将数据转化为同训练数据相同的变量)

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* cols (py3pkl): 训练数据哑编码后的所有变量

#### Output:

* d_dummy_data (py3pkl): 哑编码后的数据


## <a id="ExtratreeC">ExtratreesClasSPy3</a>
这个类实现了一个元估计器，该估计器适合于数据集的各个子样本上的多个随机决策树（又名extra-trees），并使用平均值来提高预测准确度和控制过度拟合。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="ExtratreeR">ExtratreesRegrSPy3</a>
这个类实现了一个元估计器，该估计器适合于数据集的各个子样本上的多个随机决策树（又名extra-trees），并使用平均值来提高预测准确度和控制过度拟合。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 评估算法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="FNAD">FillNADataSPy3</a>
将指定变量的缺失值全部填补为0

#### Tag:

* dataframe

#### Param:

* cols: string

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl)： 特定列缺失填补完的数据


## <a id="FShowU">FormShowUnivSPy3</a>
以HTML形式显示DataFrame

#### Tag:

* visualization

#### Param:

* None

#### Input:

* d_data (py3pkl): pickle形式的数据

#### Output:

* d_form (html): html形式的数据


## <a id="FShowCSVU">FormShowCSVUnivSPy3</a>
以HTML形式显示DataFrame

#### Tag:

* visualization

#### Param:

* None

#### Input:

* d_data (csv): csv形式的数据

#### Output:

* d_form (html): html形式的数据


## <a id="Gboosting">GradientboostingClasSPy3</a>
和Adaboost不同，Gradient Boosting 在迭代的时候选择梯度下降的方向来保证最后的结果最好。 损失函数用来描述模型的“靠谱”程度，假设模型没有过拟合，损失函数越大，模型的错误率越高。 如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度方向上下降。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* loss (string): 损失函数
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="GboostingR">GradientboostingRegrSPy3</a>
Gradient Boosting 在迭代的时候选择梯度下降的方向来保证最后的结果最好。 损失函数用来描述模型的“靠谱”程度，假设模型没有过拟合，损失函数越大，模型的错误率越高 如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度方向上下降。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* loss (string): 损失函数
* learning_rate (double): 收敛速度

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="LogisticR">LogisticRegrSPy3</a>
logistic回归是一种广义线性回归（generalized linear model），因此与多重线性回归分析有很多相同之处。它们的模型形式基本上相同，都具有 w‘x+b，其中w和b是待求参数，其区别在于他们的因变量不同，多重线性回归直接将w‘x+b作为因变量，即y =w‘x+b，而logistic回归则通过函数L将w‘x+b对应一个隐状态p，p =L(w‘x+b), 然后根据p 与1-p的大小决定因变量的值。如果L是logistic函数，就是logistic回归，如果L是多项式函数就是多项式回归。

#### Tag:

* linear_model

#### Param:

* penalty (string): 正则化（泛化）方法 (l1, l2)
* C (double): 正则化强度 (0-10)
* solve (string): 最优化方法 (liblinear, newton-cg, lbfgs, sag, saga)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型


## <a id="MMSFitD">MinMaxScalerFitDataSPy3</a>
对连续型变量进行归一化处理, 使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素, 用于训练数据。

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 归一化后的数据
* m_minmax_model (py3pkl): 归一化训练好的模型


## <a id="MMSTransformD">MinMaxScalerTransformDataSPy3</a>
对连续型变量进行归一化处理, 使用这种缩放的目的包括实现特征极小方差的鲁棒性以及在稀疏矩阵中保留零元素，将训练集中训练好的模型用于测试数据。

#### Tag:

* preprocessing

#### Param:

* None

#### Input:

* d_data (py3pkl): 数据
* model (py3pkl): 归一化训练好的模型

#### Output:

* d_changed_data (py3pkl): 归一化后的数据


## <a id="MDropD">MissingDropDataSPy3</a>
删除几乎拥有唯一值的字段(比如单个变量最大类别占比大于95%)；删除缺失百分比大于一定比率的字段(比如类别变量大于30%，连续变量大于60%)。

### Tag:

* preprocessing

### Param:

* percent_obj (int): object型变量删除阈值
* percent_non_obj (int): 非object型变量删除阈值
* percent_unique (double): 唯一值变量删除阈值
 
### Input:

* d_data (py3pkl): 数据

### Output:

* d_changed_data (py3pkl)： 字段删除后的数据


## <a id="MFillD">MissingFillDataSPy3</a>
小比例缺失值用众数或中位数填充(例如，类别变量缺失小于10%时用众数填充，非类别变量缺失小于30%时用中位数填充)。

### Tag:

* preprocessing

### Param:

* percent_obj (int): 类别变量填充阈值
* percent_non_obj (int): 非类别变量填充阈值
 
### Input:

* d_data (py3pkl): 数据

### Output:

* d_changed_data (py3pkl): 缺失值填充后的数据


## <a id="MImputeD">MissingImputeDataSPy3</a>
用传播算法对缺失值进行填充

### Tag:

* preprocessing

### Param:

* lower_null_percent (int): 类别变量填充阈值下限
* upper_null_percent (int): 类别变量填充阈值上限 
* lower_null_percent1 (int): 非类别变量填充阈值下限
* upper_null_percent1 (int): 非类别变量填充阈值上限
 
#### Input:

* d_data (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 缺失值填充后的数据


## <a id="PLCurve">PlotLearningCurveSPy3</a>
画出学习曲线，根据训练集和测试集分数判断是否过拟合或欠拟合。

#### Tag:

* visualization

#### Param:

* n_jobs (int): 平行化运行工作的个数
* n_splits (int): 交叉验证时使用折数
* test_size (double): 验证集百分比
* model (string): 输入模型estimator, 例如GaussianNB()

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* learning_curve (jpg): 学习曲线图


## <a id="PLCBest">PlotLearningCurveSPy3_BestModel</a>
画出学习曲线，根据训练集和测试集分数判断是否过拟合或欠拟合。

#### Tag:

* visualization

#### Param:

* n_jobs (int): 平行化运行工作的个数
* n_splits (int): 交叉验证时使用折数
* test_size (double): 验证集百分比

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量
* best_model (py3pkl): 训练好的模型

#### Output:

* learning_curve (jpg): 学习曲线图


## <a id="PmmlC">PmmlClasSPy3</a>
将训练后的模型保存为PMML格式发布

#### Tag:

* utils

#### Param:

* None

#### Input:

* m_fitted_model (py3pkl): 训练好的模型

#### Output:

* m_selected_fitted_model (pmml): 将模型以pmml格式输出


## <a id="Rforest">RandomforestClasSPy3</a>
随机森林是利用多棵树对样本进行训练并预测的一种分类器，它是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 特征选择方法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* d_prob (csv): 预测概率
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="RforestR">RandomforestRegrSPy3</a>
随机森林是利用多棵树对样本进行训练并预测的一种分类器，它是一个包含多个决策树的分类器，并且其输出的类别是由个别树输出的类别的众数而定。

#### Tag:

* ensemble

#### Param:

* n_estimators (int): 评估器数量
* criterion (string): 特征选择方法

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 目标变量

#### Output:

* d_pred (csv): 预测值
* o_importance_feat (csv): 特征重要性
* m_fitted_model (py3pkl): 训练好的模型 


## <a id="REvalS">RegrEvalSPy3</a>
对回归模型进行评估（包括mae,mse,r2等）

#### Tag:

* metrics

#### Param:

* multioutput (string): 计分方法 (uniform_average, raw_values, variance_weighted)


#### Input:

* d_true (csv): 真实标签
* d_pred (csv): 预测标签

#### Output:

* o_metric (csv): 返回的模型各评估值


## <a id="RPDFCE">ReportPDFClasEvalSPy3</a>
输出分类评估报告

#### Tag:

* metrics
* visualization

#### Param:

* None

#### Input:

* o_metric (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix (jpg): 混淆矩阵图
* o_roc_curve (jpg): ROC曲线图
* o_metric_2 (csv): 各评估指标（准确率、Kappa分数、F分数、ROC值等）
* o_classification_report_2 (txt): 评估报告(F分数、精确率、召回率)
* o_confusion_matrix_2 (jpg): 混淆矩阵图
* o_roc_curve_2 (jpg): ROC曲线图

#### Output:

* evaluation_report (pdf): 评估报告


## <a id="RFEF">RFEFeatSPy3</a>
递归特征消除的主要思想是反复的构建模型（如SVM或者回归模型）然后选出最好的（或者最差的）的特征（可以根据系数来选），把选出来的特征放到一边，然后在剩余的特征上重复这个过程，直到所有特征都遍历了。

#### Tag:

* feature_selection

#### Param:

* percent_to_keep (double): 保留变量个数百分比 (0-1)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* d_changed_data (csv): 递归特征消除后的数据
* d_rfe_support (html): 统计哪些变量保留，哪些不保留
* rfe_cols (py3pkl): 筛选后保留的变量


## <a id="SplitF">SplitFeatSPy3</a>
将特征数据集和标签数据集拆分为训练集（特征、标签），测试集（特征、标签）。

#### Tag:

* feature_selection

#### Param:

* test_size (double): 测试集比例 (0-1.0)

#### Input:

* d_feature (csv): 特征变量
* d_label (csv): 标签变量

#### Output:

* d_feature_train (csv): 训练集特征变量
* d_feature_test (csv): 标签集特征变量
* d_label_train (csv): 训练集标签变量
* d_label_test (csv): 标签集特征变量


## <a id="VTFitF">VarianceThresholdFitFeatSPy3</a>
根据方差去掉取值变化小的特征，用于训练集数据。

#### Tag:

* feature_selection

#### Param:

* None

#### Input:

* d_feature (py3pkl): 数据

#### Output:

* d_changed_data (py3pkl): 方差筛选后的数据
* model (py3pkl): 方差筛选训练好的模型


## <a id="VTTransformF">VarianceThresholdTransformFeatSPy3</a>
根据方差去掉取值变化小的特征，使用训练集训练好的模型于测试集数据。

#### Tag:

* feature_selection

#### Param:

* None

#### Input:

* d_feature (py3pkl): 数据
* model (py3pkl): 方差筛选训练好的模型

#### Output:

* d_changed_data (py3pkl): 方差筛选后的数据



## Hive_To_Dataframe

hive表转dataframe

#### Param:

* delimiter: 数据分割符

* table_name: hive表名

#### Input:

* jdbc_url: hive表地址

#### Output:

* dataframe: 输出的df


## FromShow:查看dataframe

### Param:

* head: 显示的行数

### Input:

* dataframe: 要查看的dataframe

### Output:

* Table: 绘制的数据表

## DataPreprocess:数据预处理

### Param:

* frac: 采样百分比如0.1, 0.5 '#'表示取全量
* drop_columns: 待删除的字段如"x1",x2"
* LabelEncode: 要编码的分类字段

### Input:

* dataframe: 待处理的数据

### Output:

* df: 处理后的数据


## select:字段选择

### Param:

* col_name: 按列名选如"x1","x2","x3"
* col_no: 按行号选

### Input:

* data_source: 源数据

### Output:

* columns: 挑选出的字段

## LogisticRegression:逻辑回归

### Param:

* model_save_path: 模型保存路径
* model_name: 模型名称
* CV: 交叉验证次数
* metrics: 模型评估指标:"accuracy","f1_micro","f1_macro"

### Input:

* X: 自变量
* Y: 预测变量

### Output:

* model: 训练出的模型

## DecisionTreeClassifiter:决策树分类

### Param:

* CV: 交叉验证次数
* criterion: The function to measure the quality of a split. eg:gini
* import_feature: 返回的重要特征的阈值(< 1)如0.01
* min_sample_leaf: The minimum number of samples required to be at a leaf node
* max_depth: 最大深度
* metrics: 模型评估指标:"accuracy","f1_micro","f1_macro"
* model_save_path: 模型保存路径
* model_name: 模型名称
* min_samples_split: The minimum number of samples required to split an internal node

### Input:

* X: 自变量
* Y: 预测变量

### Output:

* model: 训练出的模型
* TreeShow: 决策树结构图
* import_X: 重要的特征变量

### MetriceShow:模型指标查看

### Input:

* model: 模型

### Output:

* metrics: 评估指标
* metrics_desribe: 评估指标统计

## ModelSelect:模型选择

### Param:

* metric: 评价指标

### Input:

* model_A: 模型
* model_B: 模型

### Output:

* better_model: 选出的最优模型

## Predict:预测

### Param:

* result_col_name: 预测结果的字段名

### Input:

* model: 模型
* X: 自变量

### Output:

* Y: 预测变量

## ResultEvaluation:预测结果评估

### Param:
* FigureType: 评估方法 如：roc

### Input:

* Real: 真实值
* Predict: 预测值

### Output:

* Show: 评估结果

## df2hive:dataframe存入hive

### Param:

* jdbc_url: hive连接串
* table_name: hive表名

### Input:

* data: 目标数据

## data_describe:字段统计

### input:

* data: 输入的dataframe

### output:

* describes: 数据描述信息

## imputer:缺失值填充

### input:

* data: 输入的dataframe

### output:

* data_new: 填充缺失值后的dataframe

## <a id="he">HashingEncoder</a>

分类变量哈希编码

## Tag:

* preprocessing

### Param:

* columns: 要编码的变量 如: x1,x2,x3 列名

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe


## OneHotEncoder:onehot编码

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## OrdinalEncoder:不知道中文叫啥

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## BinaryEncoder

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## PolynomialEncoder

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## BackwardDifferenceEncoder

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## SumEncoder

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## HelmertEncoder

### Param:

* columns: 要编码的变量

### Input:

* data: 输入的dataframe

### Output:

* data_new: 编码后的dataframe

## <a id="Ada">AdaBoost</a>
建立AdaBoost集成算法模型并使用训练集训练

### Tag:

* ensemble

### Param:

* learning_rate: 学习速率
* n_estimators: 训练模型时的最大因子数量

### Input:

* x_train: 输入的自变量
* y_train: 输入的目标变量

### Output:

* adaboost_classifier: 训练后的adaboost模型

## <a id="xg">xgboost::ensemble</a>
集成算法

### Param:
* None

### Input:

* x_train: 输入的自变量
* y_train: 输入的目标变量

### Output:

* xgboost_classifier: 训练后的xgboost模型

## <a id="sc">StandardScaler::preprocessing</a>
数据标准化

### Param:

* None

### Input:

* df: 输入的dataframe

### Output:

* df_new: 标准化后的dataframe

## <a id="MM">MinMaxScaler::preprocessing</a>
数据归一化

### Param:
* None

### Input:

* df: 输入的dataframe

### Output:

* df_new: 归一化后的dataframe

## <a id="sfm">SelectFromModel</a>
特征选择中的一类方法（embedded嵌入类方法）。该方法是基于机器学习模型对特征进行打分的方法。

### Tag:

* feature_selection

### Param:

* None

### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* select_cols: 筛选后的变量
* meta_json: 统计值
* df_support: 是否选择该变量的dataframe

## <a id="pearson">PearsonCorrelation</a>
通过皮尔森相关系数筛选变量

### Tag:

* feature_selection

### Param:
* corr_thel: 按输入阈值筛选变量

### Input:

* x: 输入的自变量

### Output:
* x_new: 相关性筛选后的自变量dataframe
* heatmap: 热力图

## <a id="rfe">RFE</a>
递归特征消除法(Recursive Feature Elimination): 一种特征选择方法，基于算法输出的变量系数或者特征重要性，逐步地删除重要性小的变量。

### Tag:

* feature_selection

### Param:

* step: 筛选时步长

### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* rfe_columns: 特征选择后的自变量
* meta_json: 统计量
* df_rfe: 是否选择变量矩阵

## <a id="mine">MINE::feature_selection</a>
最大信息系数(MIE)用于衡量两个变量线性或非线性的强度

### Param:

* type: 计算自变量间或是自变量与目标变量间的最大信息系数

### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* mic: 最大信息系数
* tic: 总信息系数

## <a id="chi">chi2</a>
特征选择方法，计算自变量与目标变量间的卡方统计量

### Tag:

* feature_selection

### Param:

* sample_rate: 抽样样本比例
* percent: 保留变量个数百分比

### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* x_new: 特征选择后的自变量
* y_new: 同y
* stat: 卡方统计后各变量的卡方分数和p值

## <a id="ft">FunctionTransformer::preprocessing</a>
将x传递给用户自定义的函数，并返回此函数的结果

### Param:

* None

### Input:

* x: 输入的dataframe

### Output:

* x_new: 转换后的dataframe

## <a id="poly">PolyNomialFeaturs</a>
生成多项式和交互变量

### Tag:

* preprocessing

### Param:

* degree: 多项式特征的程度
* interaction_only: 是否只包含交互项

### Input:

* x: 输入的dataframe

### Output:

* x_new: 转换后的dataframe

## <a id="sample">Sample</a>
对数据进行抽样

### Tag:

* preprocessing

### Param:

* frac: 抽样比例

### Input:

* df: 输入的dataframe

### Output:

* df_new: 抽样后的dataframe

## <a id="VC">ValueCounts</a>
统计单个变量每一类的数量

### Tag:

* dataframe

### Param:

* col: 要统计的变量名
 
### Input:

* df: 输入的dataframe

### Output:

* count: 统计结果

## <a id="BLF">BucketLowFrequency</a>
对类别变量进行处理：对单个变量中数量较少的类(百分比小于0.05)合并成一类，统一赋值为99，该步骤应在对变量进行编码之后进行。

### Tag:

* dataframe

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 转换后的dataframe

## <a id="VS">VariablesSelection</a>
选取我们认为对客户流失行为有影响的变量。

### Tag:

* dataframe

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 处理后的dataframe

## <a id="DP">DataPreprocessing</a>
对变量进行预处理：转换错误的变量类型；用0填补部分变量的缺失值(一些变量根据业务定义可以用0进行填补，比如交易金额)；对一些变量进行加工处理

### Tag:

* preprocessing

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 处理后的dataframe


## <a id="DataT">DataTypes</a>
探查数据类型

### Tag:

* dataframe

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* dtypes: 每个变量的数据类型


## <a id="MCheck">MissingCheck</a>
统计变量缺失百分比并以柱形图显示。

### Tag:

* dataframe

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_null: 缺失值百分比统计
* percent_plot: 缺失值百分比柱形图

## <a id="AsT">AsType</a>
转换变量类型

### Tag:

* preprocessing

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 转换后的dataframe
* type: 检查变量数据类型

## <a id="classmap">ClassMapping</a>
将是字符的类别型变量映射为数值, 缺失值仍保持为np.nan。注意：类别型变量若已为数值则不做转换。

### Tag:

* preprocessing

### Param:

* cols: 待转换的变量
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 转换后的dataframe


## <a id="QTrans">QuantileTransformer</a>
对连续变量进行正态化处理。

### Tag:

* preprocessing

### Param:

* None
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 转换后的dataframe

## <a id="box">Box</a>
对连续变量进行分箱处理。

### Tag:

* preprocessing

### Param:

* box_type：按区间分段还是按分位数分段
* box_num：分箱个数
 
### Input:

* df: 输入的dataframe

### Output:

* df_new: 分箱后的dataframe

## <a id="split">SplitXY</a>
将自变量和目标变量分开

### Tag:

* dataframe

### Param:

* target：目标变量
 
### Input:

* df: 输入的dataframe

### Output:

* X: 自变量
* y: 因变量

## <a id="union">Union</a>
对RFE和SelectFromModel进行特征选择后的变量进行合并(取并集)。

### Tag:

* feature_selection

### Param:

* None
 
### Input:

* x: 输入的自变量
* y: 输入的因变量
* rfe_cols: 递归特征消除筛选的变量
* select_cols: 极端树模型筛选的变量

### Output:

* x_new: 筛选后的自变量
* y_new: 因变量
* meta_json: 统计值

## <a id="imba">Imbalance</a>
处理不均衡数据

### Tag:

* preprocessing

### Param:

* None
 
### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* x_new: 处理后的自变量
* y_new: 处理后的因变量
* meta_json: 统计值

## <a id="split">TrainTestSplit</a>
将数据分为训练集和测试集

### Tag:

* model_selection

### Param:

* test_size: 测试集所占比例
 
### Input:

* x: 输入的自变量
* y: 输入的因变量

### Output:

* xtrain: x训练集
* xtest: x测试集
* ytrain: y训练集
* ytest: y测试集

## <a id="stack">Stacking</a>
堆栈模型：分为两层，第一层是几个模型的集合，第二层是单独的一个模型，用第一层几个模型的输出作为第二层的输入来训练元模型。

### Tag:

* ensemble

### Param:

* None
 
### Input:

* xtrain: x训练集
* xtest: x测试集
* ytrain: y训练集
* ytest: y测试集

### Output:

* model: 训练好的模型
* meta_json: 评估值

## <a id="cnf">ConfusionMatrix</a>
混淆矩阵

### Tag:

* metrics

### Param:

* None
 
### Input:

* model: 训练好的模型
* xtest: x测试集
* ytest: y测试集

### Output:

* cnf_matrix: 混淆矩阵
* report: 各个指标报告
* cnf_matrix_plot: 混淆矩阵图

## <a id="pred">Prediction</a>
用训练好的模型生成预测结果

### Tag:

* metrics

### Param:

* None
 
### Input:

* model: 训练好的模型
* xtest: x测试集
* ytest: y测试集

### Output:

* meta_json: 评估值
* pred: 模型预测类别
* prob: 模型预测概率

## <a id="GetSpark">GetSparkSessionUnivDPy3</a>
通过rest接口访问livy获取SparkSession

### Tag:

* pyspark

### Param:

* executorMemory：executor内存
* queue：队列
* drivermemory：driver内存
* kind：类型
* numExecutors：executor个数
* driverCores：driver核数
* host：主机地址
 
### Input:

* None

### Output:

* o_session: 启动session的url

## <a id="FSSpark">FeatureSelectorDataDPy3</a>
在Spark中选择数据指定的特征

### Tag:

* pyspark
* dataframe

### Param:

* inclodeCol: 需要的变量
* excoldeCol: 不需要的变量

### Input:

* o_session: spark session路径
* d_srcdf：输入的数据(HDFS或Hive)

### Output:

* d_dstdf: 输出数据路径 

## <a id="FTSpark">FeatureTransformerFeatDPy3</a>
在Spark中对数据进行特征转换

### Tag:

* pyspark
* preprocessing

### Param:

* inclodeCol: 需要的变量
* excoldeCol: 不需要的变量
* transformType: 转换方法

### Input:

* o_session: spark session路径
* d_srcdf：输入的数据(HDFS或Hive)

### Output:

* d_dstdf: 输出转换后的数据路径

 
## <a id="DTtrainSpark">DecisionTreeClasDPy3</a>
在Spark中训练决策树模型

### Tag:

* pyspark
* tree

### Param:

* seed：定义随机种子
* summary_path：模型评估指标存放路径
* impurity：不纯度
* featuresCols：自变量
* maxDepth：最大深度
* minInstancesPerNode：最小叶节点个数
* model_path：模型存放路径
* minInfoGain：最小信息增益
* labelCol：因变量
* testRate：测试集比例
* maxBins：最大箱数

### Input:

* o_session: spark session路径
* d_modeldata：输入放入模型训练的数据

### Output:

* o_model_path：输出训练好的模型路径

## <a id="DTevalSpark">DecisionTreeEvalDPy3</a>
在Spark中用训练好的决策树模型预测新数据

### Tag:

* pyspark
* tree

### Param:

* featuresCol: 用于预测模型的自变量
* result_path：预测结果存放路径

### Input:

* o_session: spark session路径
* d_predict_data：预测数据
* o_model_path：训练好模型的存放路径

### Output:

o_session_url：spark session路径

## <a id="CloseSpark">CloseSparkSessionUnivDPy3</a>
关闭SparkSession

### Tag:

* pyspark

### Param:

* None

### Input:

* session: spark session路径

### Output:

* None

