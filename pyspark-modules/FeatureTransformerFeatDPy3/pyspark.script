from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, IndexToString, QuantileDiscretizer

##############################read data##################################
if "{{file_type}}" == "HDFS":
    srcdf = spark.read.csv("{{file_path}}", header=True, inferSchema=True)
elif "{{file_type}}" == "HIVE":
    srcdf = spark.table("{{file_path}}")
##############################selct features###############################
selectCols = []
allCol = "*"
noCol = "#"
exclodeCol = [{{exclodeCol}}]
inclodeCol = [{{inclodeCol}}]

if inclodeCol[0] != allCol and exclodeCol[0] == noCol:
    selectCols = [{{inclodeCol}}]
elif inclodeCol[0] == allCol and exclodeCol[0] != noCol:
    for col in srcdf.columns:
        if col not in exclodeCol:
            selectCols.append(col)
elif inclodeCol[0] == allCol and exclodeCol[0] == noCol:
    selectCols = srcdf.columns

##############################transform data###############################
tmpCols = []
if "{{transformType}}" == "QuantileDiscretizer":
    quantile = QuantileDiscretizer()
    for col in selectCols:
        srcdf = quantile.setInputCol(col).setOutputCol("tmp" + col).setNumBuckets(4).fit(srcdf).transform(srcdf)
        tmpCols.append("tmp" + col)
elif "{{transformType}}" == "StringIndexer":
    indexer = StringIndexer()
    for col in selectCols:
        srcdf = indexer.setInputCol(col).setOutputCol("tmp" + col).fit(srcdf).transform(srcdf)
        tmpCols.append("tmp" + col)
elif "{{transformType}}" == "VectorAssembler":
    for col in selectCols:
        tmpCols.append("tmp" + col)
    assembler = VectorAssembler(inputCols=selectCols, outputCol="features")
    srcdf = assembler.transform(srcdf)

if "{{transformType}}" != "VectorAssembler":
    for col in selectCols:
        srcdf = srcdf.drop(col)
    dstdf = reduce(lambda srcdf, idx: srcdf.withColumnRenamed(tmpCols[idx], selectCols[idx]), xrange(len(tmpCols)), srcdf)
elif "{{transformType}}" == "VectorAssembler":
    dstdf = srcdf
##############################transform data###############################
dstdf.write.csv(path="{{tmp_path}}", header=True)
