from pyspark.sql.types import StructType, StructField
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, IndexToString
from pyspark.sql import SparkSession
from pyspark.ml.classification import NaiveBayes
import json
from pyspark.sql.types import DoubleType, IntegerType, StringType

##############################read data##################################
if "{{file_type}}" == "HDFS":
    schema = StructType([
    StructField("id", DoubleType()),
    StructField("age", DoubleType()),
    StructField("job", StringType()),
    StructField("marital", StringType()),
    StructField("education", StringType()),
    StructField("default", StringType()),
    StructField("balance", DoubleType()),
    StructField("housing", StringType()),
    StructField("loan", StringType()),
    StructField("contact", StringType()),
    StructField("day", DoubleType()),
    StructField("month", StringType()),
    StructField("duration", DoubleType()),
    StructField("campaign", DoubleType()),
    StructField("pdays", DoubleType()),
    StructField("previous", DoubleType()),
    StructField("poutcome", StringType()),
    StructField("y", StringType())
    ])
    srcdf = spark.read.csv("{{file_path}}", header=True, schema=schema)
elif "{{file_type}}" == "HIVE":
    srcdf = spark.table("{{file_path}}")
##############################selct features###############################

selectCols = []
allCol = "*"
noCol = "#"
exclodeCol = [{{exclodeCol}}]
inclodeCol = [{{inclodeCol}}]

if inclodeCol[0] != allCol and exclodeCol[0] == noCol:
    selectCols = [{{inclodeCol}}]
elif inclodeCol[0] == allCol and exclodeCol[0] != noCol:
    for col in srcdf.columns:
        if col not in exclodeCol:
            selectCols.append(feature)
elif inclodeCol[0] == allCol and exclodeCol[0] == noCol:
    selectCols = srcdf.columns

dstdf = srcdf.select(selectCols)
##############################save features###############################
dstdf.write.csv(path="{{tmp_path}}", header=True)